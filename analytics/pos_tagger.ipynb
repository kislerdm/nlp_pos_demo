{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import functools\n",
    "import json\n",
    "from typing import Tuple, List, Union\n",
    "import pandas\n",
    "from io import StringIO\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_conllu_to_dict(document: str,\n",
    "                          meta_description: dict = {}) -> List[dict]:\n",
    "    \"\"\"CoNLL-U file parser.\n",
    "    \n",
    "    See format details here: https://universaldependencies.org/format.html\n",
    "    \n",
    "    Args:\n",
    "      document: Sentences string to parse.\n",
    "      meta_description: Metadata tags.\n",
    "                      If provided, it must contain 'tags' and 'delimiter', i.e.:\n",
    "                      meta_description = {\n",
    "                          \"tags\": [\"tag1\", \"tag2\"],\n",
    "                          \"delimiter\": \"=\",\n",
    "                      }\n",
    "    \n",
    "    Returns:\n",
    "      Dict with sentences, UD tokens.\n",
    "    \"\"\"\n",
    "    _comment_line = '#'\n",
    "    _sentence_delimiter = '\\n\\n'\n",
    "    _col_delimiter = '\\t'\n",
    "    _col_names = [\"id\", \"form\", \"lemma\", \n",
    "                  \"upos\", \"xpos\", \n",
    "                  \"feats\", \"head\", \"deprel\", \n",
    "                  \"deps\", \"misc\"]\n",
    "    _na_char = '_'\n",
    "    _feat_delimiter = '='\n",
    "    _feats_delimiter = '|'\n",
    "    \n",
    "    \n",
    "    if meta_description:\n",
    "        def _meta_extractor(string: str) -> str:\n",
    "            \"\"\"Helper function to extract meta data string.\"\"\"\n",
    "            return string.split(meta_description['delimiter'])[1].strip()\n",
    "\n",
    "\n",
    "    def _feat_parser(feats: str) -> dict:\n",
    "        \"\"\"Feature parser from string into dict.\"\"\"\n",
    "        if feats == _na_char:\n",
    "            return _na_char\n",
    "        return {\n",
    "            i[0]: i[1] \n",
    "            for i in [feat.split(_feat_delimiter)[:2]\n",
    "                      for feat in feats.split(_feats_delimiter)]\n",
    "        }\n",
    "    \n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for sentence in document.split(_sentence_delimiter):\n",
    "        if sentence == \"\":\n",
    "            break\n",
    "        data = []\n",
    "        meta = {}\n",
    "        for line in sentence.split('\\n'):\n",
    "            if not line.startswith(_comment_line):\n",
    "                data.append(line)\n",
    "            else:\n",
    "                if not meta_description:\n",
    "                    continue\n",
    "                else:\n",
    "                    comment_tags = meta_description['tags'].copy()\n",
    "                    for comment_tag in comment_tags:\n",
    "                        if comment_tag in line:\n",
    "                            meta[comment_tag] = _meta_extractor(line)\n",
    "                            comment_tags.remove(comment_tag)\n",
    "        \n",
    "        data_str = '\\n'.join(data)\n",
    "        del data\n",
    "\n",
    "        with StringIO(data_str.replace('\"', '\\\\\"')) as data:\n",
    "            df = pandas.read_csv(data, \n",
    "                                 delimiter=_col_delimiter, \n",
    "                                 header=None,\n",
    "                                 quotechar='\"',\n",
    "                                 names=_col_names)\n",
    "        \n",
    "        tokens = df.to_dict('records')\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                token['feats'] = _feat_parser(token['feats'])\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                continue\n",
    "        \n",
    "        out = {\n",
    "            \"tokens\": tokens,\n",
    "        }\n",
    "        if meta:\n",
    "            out['meta'] = meta\n",
    "        \n",
    "        output.append(out)\n",
    "    \n",
    "    return output  \n",
    "\n",
    "\n",
    "parser = functools.partial(parser_conllu_to_dict, \n",
    "                           meta_description={\n",
    "                               \"tags\": [\"sent_id\", \"text\", \"s_tape\"],\n",
    "                               \"delimiter\": \"=\",\n",
    "                           })\n",
    "\n",
    "\n",
    "def corpus_parser(path: str) -> Union[Tuple[List[dict], None],\n",
    "                                      Tuple[list, str]]:\n",
    "    \"\"\"Function to read corpus from conllu corpus file.\n",
    "    \n",
    "    Args:\n",
    "      path: File path.\n",
    "    \n",
    "    Returns:\n",
    "      List of token trees with error string in case of any.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(path):\n",
    "        return [], f\"File {path} doesn't exist.\"\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            document = f.read()\n",
    "        return parser(document), None\n",
    "    except Exception as ex:\n",
    "        return [], ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_DATA = \"/transfer/data/UD_English-GUM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_TRAIN = f\"{BUCKET_DATA}/en_gum-ud-train.conllu\"\n",
    "FILE_TEST = f\"{BUCKET_DATA}/en_gum-ud-test.conllu\"\n",
    "FILE_DEV = f\"{BUCKET_DATA}/en_gum-ud-test.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tree_train, err = corpus_parser(FILE_TRAIN)\n",
    "if err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tree_test, err = corpus_parser(FILE_TEST)\n",
    "if err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tree_dev, err = corpus_parser(FILE_DEV)\n",
    "if err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare tokens to be consumed by NLTK\n",
    "def conllu_dict_to_nltk(tokens_list: List[dict]) -> List[List[Tuple[str]]]:\n",
    "    \"\"\"Function to convert conllu dict to the NLTK requred sturucture.\"\"\"\n",
    "    return [[(token['form'], token['upos']) \n",
    "             for token in tokens['tokens']]\n",
    "            for tokens in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "georgetown_tagged_sents_train = conllu_dict_to_nltk(tokens_tree_train)\n",
    "georgetown_tagged_sents_dev = conllu_dict_to_nltk(tokens_tree_dev)\n",
    "georgetown_tagged_sents_test = conllu_dict_to_nltk(tokens_tree_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-base tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "    (r\"^(an|a|the)$\", \"DET\"),\n",
    "    (r\"^(of|in|to|for|on|with|at|from|by|inside|outside)$\", \"ADP\"),\n",
    "    (r\"^(also|so|then|just|more|as|very|well|even|most)$\", \"ADV\"),\n",
    "    (r\"^(and|or|but|\\&|both|either|nor|so|though|although|however)$\", \"CCONJ\"),\n",
    "    (f\"^(yes|jup|yeah|yey|well|no|neh|meh|oh|yeah|hey|okay|yep|OK)$\", \"INTJ\"),\n",
    "    (f\"^(that|if|when|as|how|where|because|while|after)$\", \"SCONJ\"),\n",
    "    (r\"^(\\.|\\;|\\:|\\,|\\'|\\\"|\\\"\\\"|\\''|\\]|\\[|\\(|\\)|\\?|\\!)$\", \"PUNCT\"),\n",
    "    (r\"^(\\\\|``|`|#|@|%|\\$)$\", \"SYM\"),\n",
    "    (r\"^-?[0-9]+(\\.[0-9]+)?$\", \"NUM\"),\n",
    "    (r\"^[a-zA-Z0-9\\.\\-]+@[a-zA-Z0-9\\.\\-]+\\.[a-zA-Z]+$\", \"PRON\"),\n",
    "    (r\"(.*ing|.*ish)$\", \"ADJ\"),\n",
    "    (f\"^(.*es|.*ed)$\", \"VERB\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = nltk.DefaultTagger('NOUN')\n",
    "t_rules = nltk.RegexpTagger(rules, backoff=t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19831700577744285"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the most simplistic approach is, to assume that all words are nouns\n",
    "\n",
    "t0.evaluate(georgetown_tagged_sents_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumption simply reflets dataset composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5362346144184879"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_rules.evaluate(georgetown_tagged_sents_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rule-based approach gives the accuracy higher than **50%** which is better than random guessing, or than assuming all words being *nouns*.\n",
    "\n",
    "This approach doesn't include any machine learning though, so let's turn it on and improve PoS tagger accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-gram tagger\n",
    "\n",
    "1-gram tagger scans through the data set and defines the tag which is being assigned to a given token in most of the cases. This model can be assigned to the *lexical based* tagging method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan the train sample and train the tagger\n",
    "# let's also add the t0 (NOUN by default) tagger as the fallback solution\n",
    "\n",
    "t_1gram = nltk.UnigramTagger(georgetown_tagged_sents_train, backoff=t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8401155488570711"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1gram.evaluate(georgetown_tagged_sents_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-gram tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the above illustrated models include sentese context into account. Let's fix that by looking up the PoS of the previous token for every token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 2-gram tagger with the t_1gram as the fallback\n",
    "\n",
    "t_2gram = nltk.BigramTagger(georgetown_tagged_sents_train, backoff=t_1gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8468977643808089"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_2gram.evaluate(georgetown_tagged_sents_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem to give much gain compared to 1-gram tagger. Let's include one more token to be looked back during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-gram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_3gram = nltk.BigramTagger(georgetown_tagged_sents_train, backoff=t_2gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8468977643808089"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_3gram.evaluate(georgetown_tagged_sents_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we hit the wall and increase of the lookback window won't make any positive impact on the model accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
